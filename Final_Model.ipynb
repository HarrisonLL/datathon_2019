{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'4/12/2020'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5d1af0932ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_comp_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_date\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mreal_x_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_comp_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_date\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mreal_y_train_st\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_comp_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchangedate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ST'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mreal_y_train_mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_comp_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchangedate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mreal_y_train_lt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_comp_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchangedate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '4/12/2020'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "import csv\n",
    "import datetime\n",
    "from nltk import tokenize\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "wordList = np.load('wordList.npy')\n",
    "wordList = wordList.tolist()\n",
    "print('Loaded the word list!')\n",
    "wordVectors = np.load('vectorList.npy')\n",
    "\n",
    "maxSequence = 100\n",
    "numDimension = 50\n",
    "\n",
    "punctuation = [',','.',':','(',')','!','?','\"','“','”']\n",
    "# text 以天为单位\n",
    "\n",
    "\n",
    "\n",
    "def delete_stop_words(text): # this text is the training data for a certain day\n",
    "    stop_words = set(stopwords.words('english'))       \n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in word_tokens if not w in stop_words]       \n",
    "    filtered_text = []       \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words and w not in punctuation: \n",
    "            filtered_text.append(w) \n",
    "    return filtered_text\n",
    "\n",
    "def construct_big_sentence(filtered_text):\n",
    "    big_sentence_bag = [] # text for 1 day, a list of list, each sublist has length 200\n",
    "    counter = 0\n",
    "    for word in filtered_text:\n",
    "        counter += 1\n",
    "        if counter == maxSequence:\n",
    "            big_sentence = filtered_text[:maxSequence]\n",
    "            big_sentence_bag.append(big_sentence)\n",
    "        elif counter % maxSequence == 0 and counter != len(filtered_text):\n",
    "            start = (int(counter / maxSequence) - 1) * maxSequence\n",
    "            big_sentence = filtered_text[start : start + maxSequence]\n",
    "            big_sentence_bag.append(big_sentence)\n",
    "        elif counter == len(filtered_text):\n",
    "            start = int(counter / maxSequence) * maxSequence\n",
    "            big_sentence = filtered_text[start : ]\n",
    "            big_sentence_bag.append(big_sentence)    \n",
    "    return (big_sentence_bag)\n",
    "\n",
    "def sentence_index(big_sentence_bag):\n",
    "    big_sentence_bag_index = []\n",
    "    for big_sentence in big_sentence_bag:\n",
    "        big_sentence_index = []\n",
    "        for word in big_sentence:\n",
    "            if word in wordList:\n",
    "                wordindex = wordList.index(word.lower())\n",
    "                big_sentence_index.append(wordindex)\n",
    "            else:\n",
    "                pass\n",
    "        big_sentence_bag_index.append(big_sentence_index)\n",
    "    return big_sentence_bag_index\n",
    "\n",
    "def everyday_sentence_matrix_bag(big_sentence_bag_index):\n",
    "    daily_sentence_matrix_bag = []\n",
    "    for shortText in big_sentence_bag_index:\n",
    "        shortTextMatrix = []\n",
    "        if len(shortText) == maxSequence:            \n",
    "            for word_index in shortText:\n",
    "                shortTextMatrix.append(wordVectors[word_index])\n",
    "        else:\n",
    "            zero = [0 for i in range(numDimension)]\n",
    "            diff = maxSequence - len(shortText)\n",
    "            for word_index in shortText:\n",
    "                shortTextMatrix.append(wordVectors[word_index])\n",
    "            for i in range(diff):\n",
    "                shortTextMatrix.append(zero)\n",
    "        daily_sentence_matrix_bag.append(shortTextMatrix)\n",
    "    return np.array(daily_sentence_matrix_bag)\n",
    "################ 代码调试区域\n",
    "def isLineEmpty(line):\n",
    "    return len(line.strip()) == 0\n",
    "\n",
    "def read_text(file):\n",
    "    text = \"\"\n",
    "    new_text = \"\"\n",
    "    with open (file,'r') as f:\n",
    "        for line in f:\n",
    "            if isLineEmpty(line) == False:\n",
    "                text+=line\n",
    "    f.close()\n",
    "    print(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        new_text += (sentence + ' ')\n",
    "    return new_text\n",
    "\n",
    "def get_them(text):\n",
    "    filtered_text = delete_stop_words(text)\n",
    "    big_sentence_bag = construct_big_sentence(filtered_text)\n",
    "    big_sentence_bag_index = sentence_index(big_sentence_bag)\n",
    "    daily_sentence_matrix_bag = everyday_sentence_matrix_bag(big_sentence_bag_index)\n",
    "    return daily_sentence_matrix_bag\n",
    "\n",
    "with open('X_train.json') as json_file:  \n",
    "    raw_x = json.load(json_file)\n",
    "json_file.close()\n",
    "\n",
    "with open('Y_train.json') as json_file:\n",
    "    Y_train = json.load(json_file)\n",
    "json_file.close()\n",
    "\n",
    "X_train = raw_x\n",
    "for key_comp_name in raw_x.keys():\n",
    "    for key_date in raw_x[key_comp_name].keys():\n",
    "        n_d_array = get_them(raw_x[key_comp_name][key_date])\n",
    "        X_train[key_comp_name][key_date] = n_d_array\n",
    "\n",
    "real_x_train = []\n",
    "real_y_train_st= []\n",
    "real_y_train_mt= []\n",
    "real_y_train_lt= []\n",
    "\n",
    "def changedate(date):\n",
    "    d_list = date.split('/')\n",
    "    yr = d_list[0]\n",
    "    mt = d_list[1]\n",
    "    day = d_list[2]\n",
    "    return str(int(mt))+'/'+day+'/20'+yr\n",
    "\n",
    "for key_comp_name in X_train.keys():\n",
    "    for key_date in X_train[key_comp_name].keys():\n",
    "        #if X_train[key_comp_name][key_date].shape[0]>1:\n",
    "        for i in range(X_train[key_comp_name][key_date].shape[0]):\n",
    "            real_x_train.append(X_train[key_comp_name][key_date][i])\n",
    "            real_y_train_st.append(Y_train[key_comp_name][changedate(key_date)]['ST'])\n",
    "            real_y_train_mt.append(Y_train[key_comp_name][changedate(key_date)]['MT'])\n",
    "            real_y_train_lt.append(Y_train[key_comp_name][changedate(key_date)]['LT'])\n",
    "# create and fit the LSTM network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(LSTM(units=50, return_sequences=True, input_shape=(100,50)))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(LSTM(units=50))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(1))\n",
    "print('hi')\n",
    "model1.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model1.fit(real_x_train, real_y_train_st, epochs=2, batch_size=10, verbose=2)\n",
    "\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(units=50, return_sequences=True, input_shape=(100,50)))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(units=50))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1))\n",
    "print('hi')\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model2.fit(real_x_train, real_y_train_mt, epochs=2, batch_size=10, verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(units=50, return_sequences=True, input_shape=(100,50)))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(LSTM(units=50))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(1))\n",
    "\n",
    "print('hi')\n",
    "model3.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model3.fit(real_x_train, real_y_train_lt, epochs=2, batch_size=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = real_x_train[-10:]\n",
    "y_test_st = real_y_train_st[-10:]\n",
    "y_test_mt = real_y_train_mt[-10:]\n",
    "y_test_lt = real_y_train_lt[-10:]\n",
    "\n",
    "\n",
    "# inputs = new_data.values\n",
    "# print(inputs)\n",
    "# print(x_train.shape[0])\n",
    "# print((x_train.shape[0],len(inputs)-60-timeFrame))\n",
    "# X_test = []\n",
    "# for i in range(x_train.shape[0],len(inputs)-60-timeFrame):\n",
    "#     X_test.append(inputs[i:i+60,0])\n",
    "# X_test = np.array(X_test)\n",
    "# print(X_test.shape)\n",
    "# X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "# print(X_test.shape)\n",
    "\n",
    "y_st = model1.predict(x_test)\n",
    "y_mt = model2.predict(x_test)\n",
    "y_lt = model3.predict(x_test)\n",
    "\n",
    "print(y_st)\n",
    "print(y_mt)\n",
    "print(y_lt)\n",
    "\n",
    "for key_comp_name in X_train.keys():\n",
    "    for key_date in X_train[key_comp_name].keys():\n",
    "        #if X_train[key_comp_name][key_date].shape[0]>1:\n",
    "        for i in range(X_train[key_comp_name][key_date].shape[0]):\n",
    "            real_x_train.append(X_train[key_comp_name][key_date][i])\n",
    "            real_y_train_st.append(Y_train[key_comp_name][key_date]['ST'])\n",
    "            real_y_train_mt.append(Y_train[key_comp_name][key_date]['MT'])\n",
    "            real_y_train_lt.append(Y_train[key_comp_name][key_date]['LT'])\n",
    "# create and fit the LSTM network\n",
    "\n",
    "# the \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
